{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured data from Forums\n",
    "\n",
    "We are looking to investigate if Forum conversations can be used to extract structured data for XA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import textacy\n",
    "import nltk\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = pd.read_csv(\"data/quora.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rows.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def recognizePOS(parsed, POS) :\n",
    "    words = []\n",
    "    for token in parsed :\n",
    "        if token.pos_ == POS :\n",
    "            word = token.orth_\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def extractEntities(text):   \n",
    "    text = text.lower()\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    token_list = []\n",
    "    for token in doc:\n",
    "        token_list.append(token.lemma_)\n",
    "        \n",
    "    filtered_sentence =[] \n",
    "\n",
    "    for word in token_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "\n",
    "    \n",
    "    svos = textacy.extract.subject_verb_object_triples(doc)\n",
    "    doc = nlp(\" \".join(filtered_sentence))\n",
    "    return pd.Series(data=[text, [chunk.text for chunk in doc.noun_chunks], \n",
    "                           recognizePOS(doc, \"NOUN\"),\n",
    "                           recognizePOS(doc, \"VERB\"),\n",
    "                           recognizePOS(doc, \"ADJ\"),\n",
    "                           [svo for svo in svos]\n",
    "                           ]\n",
    "                    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nRows = data.apply(lambda row : extractEntities(row[\"question1\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nRows = nRows.rename(columns={0: \"text\", 1: \"noun chunks\", 2: \"nouns\", 3: \"verbs\", 4: \"adjs\", 5: \"Sub/Verb/Obj\"})\n",
    "nRows.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = \"speed\"\n",
    "\n",
    "fRows = nRows.copy()\n",
    "fRows[\"found\"] = nRows[\"nouns\"].apply(lambda l: l.count(term))\n",
    "\n",
    "indexNames = fRows[ fRows['found'] == 0 ].index\n",
    "fRows.drop(indexNames , inplace=True)\n",
    "fRows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
